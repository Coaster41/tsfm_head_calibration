{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52e3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
      "Loaded PyTorch TimesFM, likely because python version is 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0].\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timesfm\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.itertools import batcher\n",
    "from utils.utils import load_test_data\n",
    "from utils.data_loader import create_cached_tsmixup_datasets\n",
    "from load_cached_features import *\n",
    "from timesfm.pytorch_patched_decoder import ResidualBlock\n",
    "import matplotlib.pyplot as plt\n",
    "# from utils.utils import load_test_data\n",
    "PSZ = \"auto\"  # patch size: choose from {\"auto\", 8, 16, 32, 64, 128}\n",
    "BSZ = 33  # batch size: any positive integer\n",
    "TEST = 100  # test set length: any positive integer\n",
    "context_len = 512\n",
    "pred_len = 128\n",
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb8892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c8548f54294f8084430dc367bbc270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfm = timesfm.TimesFm(\n",
    "            hparams=timesfm.TimesFmHparams(\n",
    "                backend='gpu',\n",
    "                per_core_batch_size=64,\n",
    "                context_len=context_len,  # currently max supported\n",
    "                horizon_len=pred_len,  # number of steps to predict\n",
    "                input_patch_len=32,  # fixed parameters\n",
    "                output_patch_len=128,\n",
    "                num_layers=50,\n",
    "                model_dims=1280,\n",
    "                use_positional_embedding=False,\n",
    "                point_forecast_mode='mean',\n",
    "                device=device,\n",
    "            ),\n",
    "            checkpoint=timesfm.TimesFmCheckpoint(\n",
    "                huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c35603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tsmixup dataset\n",
    "train_dataset, val_dataset = create_cached_tsmixup_datasets(\n",
    "        max_samples=300000,\n",
    "        context_length=512,\n",
    "        prediction_length=96, # 1 or 96\n",
    "        num_workers=16,\n",
    "        cache_dir=\"/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/\",\n",
    "        processed_cache_path=\"/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/tsmixup_processed_300000_512_96.pkl\",\n",
    "        batch_size=4000\n",
    "    )\n",
    "\n",
    "def load_dataset(dataset, ts=1000, pred_length=1, ctx_len=512):\n",
    "    if dataset == 'tsmixup':\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(ts) if isinstance(ts, int) else ts:\n",
    "            val_dict = val_dataset[i]\n",
    "            x.append(val_dict['past_values'])\n",
    "            y.append(val_dict['future_values'])\n",
    "        x = torch.stack(x)[:, -ctx_len:]\n",
    "        y = torch.stack(y)[:,:pred_length]\n",
    "        \n",
    "    else:\n",
    "        dataset_path = f\"/extra/datalab_scratch0/ctadler/time_series_models/ts_foundation_calibration/data/{dataset}/y_{dataset}.csv\"\n",
    "        timestamp_column = \"ds\"\n",
    "\n",
    "        data = pd.read_csv(\n",
    "            dataset_path,\n",
    "            parse_dates=[timestamp_column],\n",
    "            index_col=0\n",
    "        )\n",
    "\n",
    "        x = []\n",
    "        for id, vals in data.groupby('unique_id'):\n",
    "            x.append(torch.from_numpy(vals['y'].to_numpy(np.float32)))\n",
    "        x = torch.stack(x)\n",
    "\n",
    "    if dataset != 'tsmixup':\n",
    "        y = x[:,ctx_len:ctx_len+pred_length]\n",
    "        x = x[:,:ctx_len]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601d9a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile shape: (64, 128, 10), pred_quants: (64, 128, 10), mse: 0.0\n",
      "torch.Size([64, 16, 1280]) torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def process_transformer_output(model: timesfm.TimesFm, stats, model_output, output_patch_len, output_dim):\n",
    "    output_ts = model._model.horizon_ff_layer(model_output)\n",
    "\n",
    "    # Reshape using view\n",
    "    b, n, _ = output_ts.shape\n",
    "    output_ts = output_ts.view(b, n, output_patch_len, output_dim)\n",
    "\n",
    "    mu = stats[..., 0]\n",
    "    sigma = stats[..., 1]\n",
    "    output_ts = output_ts * sigma[:, None, None, None] + mu[:, None, None, None]\n",
    "    return output_ts[:, -1].cpu().numpy()\n",
    "    # return model.ppd._reverse_transform(output_ts, stats)\n",
    "\n",
    "x, y = load_dataset('tsmixup', 100, pred_length=pred_len, ctx_len=context_len)\n",
    "# print(x.shape)\n",
    "batch_size = 64\n",
    "context = [x[i] for i in range(batch_size)]\n",
    "# print(context)\n",
    "# print(len(context), context[0].shape)\n",
    "_, quantile_forecasts, (transformer_output, stats) = tfm.forecast(context, freq=[0] * len(context), get_stacked_transformer=True)\n",
    "transformer_output = transformer_output.to(device)\n",
    "stats = stats.to(device)\n",
    "pred_quants = process_transformer_output(tfm, stats, transformer_output, 128, 10)\n",
    "print(f\"quantile shape: {quantile_forecasts.shape}, pred_quants: {pred_quants.shape}, mse: {np.sum(quantile_forecasts - pred_quants)}\")\n",
    "print(transformer_output.shape, stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd94667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-86.9447, device='cuda:3') tensor(163.5984, device='cuda:3') tensor(57.9059, device='cuda:3')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(transformer_output.min(), transformer_output.max(), transformer_output.quantile(0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce5d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
