{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5cd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from utils.data_loader import create_cached_tsmixup_datasets\n",
    "from load_cached_features import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import einops\n",
    "import time \n",
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6845421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tsmixup dataset\n",
    "train_dataset, val_dataset = create_cached_tsmixup_datasets(\n",
    "        max_samples=300000,\n",
    "        context_length=512,\n",
    "        prediction_length=128, # 1 or 128\n",
    "        num_workers=16,\n",
    "        cache_dir=\"/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/\",\n",
    "        processed_cache_path=\"/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/tsmixup_processed_300000_512_128.pkl\",\n",
    "        batch_size=4000\n",
    "    )\n",
    "\n",
    "def load_dataset(dataset, ts=1000, pred_length=1, ctx_len=512):\n",
    "    if dataset == 'tsmixup':\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(ts) if isinstance(ts, int) else ts:\n",
    "            val_dict = val_dataset[i]\n",
    "            x.append(val_dict['past_values'])\n",
    "            y.append(val_dict['future_values'])\n",
    "        x = torch.stack(x)[:, -ctx_len:]\n",
    "        y = torch.stack(y)[:,:pred_length]\n",
    "        \n",
    "    else:\n",
    "        dataset_path = f\"/extra/datalab_scratch0/ctadler/time_series_models/ts_foundation_calibration/data/{dataset}/y_{dataset}.csv\"\n",
    "        timestamp_column = \"ds\"\n",
    "\n",
    "        data = pd.read_csv(\n",
    "            dataset_path,\n",
    "            parse_dates=[timestamp_column],\n",
    "            index_col=0\n",
    "        )\n",
    "\n",
    "        x = []\n",
    "        for id, vals in data.groupby('unique_id'):\n",
    "            x.append(torch.from_numpy(vals['y'].to_numpy(np.float32)))\n",
    "        x = torch.stack(x)\n",
    "\n",
    "    if dataset != 'tsmixup':\n",
    "        y = x[:,ctx_len:ctx_len+pred_length]\n",
    "        x = x[:,:ctx_len]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f343a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.166243\n"
     ]
    }
   ],
   "source": [
    "# load pretrain model\n",
    "model = AutoModelForCausalLM.from_pretrained('qcw2333/YingLong_110m', trust_remote_code=True,torch_dtype=torch.bfloat16).to(device).eval()\n",
    "\n",
    "# prepare input\n",
    "# batch_size, lookback_length = 1, 2880\n",
    "# seqs = torch.randn(batch_size, lookback_length).bfloat16().cuda()\n",
    "batch_size = 512\n",
    "context_len = 512\n",
    "x, y = load_dataset('tsmixup', np.random.randint(0, 10000, batch_size), pred_length=128, ctx_len=context_len)\n",
    "x = x.to(device).to(torch.bfloat16)\n",
    "# print(f\"patch_len: {model.config.patch_size}\")\n",
    "pred_len = 128\n",
    "forecast_length = 2048\n",
    "patch_len = 32\n",
    "d_model = 768\n",
    "n_quantiles = 99\n",
    "decoder_out = torch.zeros((batch_size, (pred_len)//patch_len, d_model))\n",
    "loc_scale = torch.zeros((batch_size, 2))\n",
    "token_masks = []\n",
    "def save_decoder_hook(module, input, output):\n",
    "    # print(f\"transformer out: {input[0].shape}\")\n",
    "    # print(f\"head_out shape: {output.shape}\")\n",
    "    # head_out = einops.rearrange(output,'b c (l1 l2) -> b c l1 l2', l2 = 99)\n",
    "    # print(f\"head out: {head_out.shape} masked head: {head_out[:,token_masks,:,:].shape}\")\n",
    "    # model_out = einops.rearrange(head_out[:,token_masks,:,:], 'b l c d -> b (l c) d')\n",
    "    # print(f\"model_out: {model_out.shape} model out: {model_out[:,:pred_len].shape}\")\n",
    "    # print(f\"pred mask: {context_len//patch_len},{(context_len+pred_len)//patch_len}\")\n",
    "    decoder_out[:] = input[0][:,context_len//patch_len:(context_len+pred_len)//patch_len].detach().cpu()\n",
    "\n",
    "def save_tokenizer_hook(module, input, output):\n",
    "    x, x_raw, masks, mean, std, _ = output\n",
    "    # print(f\"mean {mean.shape}, std {std.shape}\")\n",
    "    # print(f\"masks {len(masks), masks}\")\n",
    "    loc_scale[:,0] = mean.flatten().detach().cpu()\n",
    "    loc_scale[:,1] = std.flatten().detach().cpu()\n",
    "    # token_masks.extend(masks)\n",
    "\n",
    "# generate forecast\n",
    "model.lm_head.register_forward_hook(save_decoder_hook)\n",
    "model.tokenizer.register_forward_hook(save_tokenizer_hook)\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(x, future_token=forecast_length)\n",
    "print(f\"Time taken: {(time.time()-start_time):4f}\")\n",
    "\n",
    "# print(f\" output_shape {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882ebd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yinglong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
